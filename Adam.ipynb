{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_t = {\\beta}_{1} v_{t-1} + (1 - {\\beta}_{1})g_t$$\n",
    "$$ s_t = {\\beta}_{2} s_{t-1} + (1 - {\\beta}_{2})g_t^2$$\n",
    "$$\\hat{v}_{t} = \\frac{v_t}{1-{\\beta}_{1}^t} \\; and \\; \\hat{s}_{t} = \\frac{s_t}{1-{\\beta}_{2}^t}$$\n",
    "$$ {g}_t' = \\frac {\\eta \\hat{v}_t}{\\sqrt{\\hat{s}_t}+\\epsilon}$$\n",
    "$$  x_t\\leftarrow x_{t-1} - {g}_t'$$\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.NeuralNet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimizerAdam(W1,b1,W2,b2,W3,b3,dW1,dB1,dW2,dB2,dW3,dB3,vW1,vb1,vW2,vb2,vW3,vb3,sW1,sb1,sW2,sb2,sW3,sb3,lr,beta1,beta2,t):\n",
    "    momentum = [vW1,vb1,vW2,vb2,vW3,vb3]\n",
    "    second_momen = [sW1,sb1,sW2,sb2,sW3,sb3]\n",
    "    gradient = [dW1,dB1,dW2,dB2,dW3,dB3]\n",
    "    params = [W1,b1,W2,b2,W3,b3]\n",
    "    for i in range(6):\n",
    "        momentum[i] = beta1 * momentum[i] + (1-beta1) * gradient[i]\n",
    "        second_momen[i] = beta2 * second_momen[i] + (1-beta2) * np.multiply(gradient[i],gradient[i])\n",
    "        v_hat = momentum[i] / (1 - beta1**t)\n",
    "        s_hat = second_momen[i] / (1 - beta2**t)\n",
    "        g_t = np.multiply(lr * v_hat,1 / (np.sqrt(s_hat)+1e-6))\n",
    "        params[i] = params[i] - g_t\n",
    "    W1,b1,W2,b2,W3,b3 = params\n",
    "    vW1,vb1,vW2,vb2,vW3,vb3 = momentum\n",
    "    sW1,sb1,sW2,sb2,sW3,sb3 = second_momen\n",
    "    return W1,b1,W2,b2,W3,b3,vW1,vb1,vW2,vb2,vW3,vb3,sW1,sb1,sW2,sb2,sW3,sb3\n",
    "        \n",
    "def UpdateParamsAdam(W1,b1,W2,b2,W3,b3,S,Y,batch_size=16,lr=1e-4,beta1=0.9,beta2=0.999):\n",
    "    n_samples = S.shape[0]\n",
    "    idx = np.arange(n_samples)\n",
    "    np.random.shuffle(idx)\n",
    "    S,Y = S[idx],Y[idx]\n",
    "    vW1,vb1,vW2,vb2,vW3,vb3 = np.zeros_like(W1),np.zeros_like(b1),np.zeros_like(W2),np.zeros_like(b2),np.zeros_like(W3),np.zeros_like(b3)\n",
    "    sW1,sb1,sW2,sb2,sW3,sb3 = np.zeros_like(W1),np.zeros_like(b1),np.zeros_like(W2),np.zeros_like(b2),np.zeros_like(W3),np.zeros_like(b3)\n",
    "    t = 1\n",
    "    for i in np.arange(0, n_samples, batch_size):\n",
    "        begin, end = i, min(i + batch_size, n_samples)\n",
    "        s,y =  S[begin:end] , Y[begin:end]\n",
    "        O1,O2,O3 = Forward(W1,b1,W2,b2,W3,b3,s)\n",
    "        dW1,dB1,dW2,dB2,dW3,dB3 = Backward(W1,b1,W2,b2,W3,b3,s,y,O1,O2,O3,lr)\n",
    "        W1,b1,W2,b2,W3,b3,vW1,vb1,vW2,vb2,vW3,vb3,sW1,sb1,sW2,sb2,sW3,sb3 = OptimizerAdam(W1,b1,W2,b2,W3,b3,dW1,dB1,dW2,dB2,dW3,dB3,vW1,vb1,vW2,vb2,vW3,vb3,sW1,sb1,sW2,sb2,sW3,sb3,lr,beta1,beta2,t)\n",
    "        t+=1\n",
    "    return W1,b1,W2,b2,W3,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 Loss:  0.08421819504384775\n",
      "Epoch  50 Loss:  0.04793530265025331\n",
      "Epoch  100 Loss:  0.026712419547392254\n",
      "Epoch  150 Loss:  0.011866150218894514\n",
      "Epoch  200 Loss:  0.00778059584401054\n",
      "Epoch  250 Loss:  0.0055093510277443815\n",
      "Epoch  300 Loss:  0.002589819141137765\n",
      "Epoch  350 Loss:  0.0027693548712071406\n",
      "Epoch  400 Loss:  0.0029833941529570037\n",
      "Epoch  450 Loss:  0.003558220328581537\n",
      "Epoch  500 Loss:  0.0015646660026048624\n",
      "Epoch  550 Loss:  0.0016096674255810987\n",
      "Epoch  600 Loss:  0.0016431763273473652\n",
      "Epoch  650 Loss:  0.001616825541650852\n",
      "Epoch  700 Loss:  0.001393229995084289\n",
      "Epoch  750 Loss:  0.0014480834898836156\n",
      "Epoch  800 Loss:  0.0012770644750152123\n",
      "Epoch  850 Loss:  0.0007979579942583756\n",
      "Epoch  900 Loss:  0.0016239848719123038\n",
      "Epoch  950 Loss:  0.0010402948035949742\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(5000,100)\n",
    "y = np.random.rand(5000,1)\n",
    "W1,b1,W2,b2,W3,b3 = FitModel(X,y,n_iter=1000,batch_size=500,lr=1e-3,UpdateParams=UpdateParamsAdam,print_stat=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
